{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cloud_etl_nlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV4ajtm-iijA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21VIzGHIioxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETLProject\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGQc0BlXit96",
        "colab_type": "code",
        "outputId": "b162d077-2a7e-42e5-bb0c-383c2a9e3786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "from pyspark import SparkFiles\n",
        "# Load in user_data.csv from S3 into a DataFrame\n",
        "url = \"https://s3.amazonaws.com//dataviz-curriculum/day_3/ratings_and_sentiments.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"ratings_and_sentiments.csv\"), inferSchema=True, sep=',', timestampFormat=\"mm/dd/yy\")\n",
        "df.show(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----------------+----------+----------+---------+------------+---------+--------+------------+------------+----------+------------+-------------+------------+-----------+---------+----------+-------------+----------+\n",
            "|    coffee_shop_name|         review_text|           rating|num_rating|cat_rating|bool_HIGH|overall_sent|vibe_sent|tea_sent|service_sent|seating_sent|price_sent|parking_sent|location_sent|alcohol_sent|coffee_sent|food_sent|hours_sent|internet_sent|local_sent|\n",
            "+--------------------+--------------------+-----------------+----------+----------+---------+------------+---------+--------+------------+------------+----------+------------+-------------+------------+-----------+---------+----------+-------------+----------+\n",
            "|The Factory - Caf...|11/25/2016 1 chec...| 5.0 star rating |         5|      HIGH|        1|           4|        3|       0|           0|           0|         0|           0|            0|           1|          3|        0|         0|            0|         0|\n",
            "|The Factory - Caf...|12/2/2016 Listed ...| 4.0 star rating |         4|      HIGH|        1|           3|        3|       0|           0|           0|         0|           0|            0|           0|          0|        2|         0|            0|         0|\n",
            "|The Factory - Caf...|11/30/2016 1 chec...| 4.0 star rating |         4|      HIGH|        1|           2|        2|       0|           0|           3|         0|           0|            0|           0|         -1|        2|         0|            0|         0|\n",
            "|The Factory - Caf...|11/25/2016 Very c...| 2.0 star rating |         2|       LOW|        0|           1|        0|       0|           0|          -1|        -1|           0|            0|           0|          0|        0|         0|            0|         0|\n",
            "|The Factory - Caf...|12/3/2016 1 check...| 4.0 star rating |         4|      HIGH|        1|           2|        0|       0|           0|           0|         0|           3|            0|           0|          0|        0|         0|            0|         0|\n",
            "|The Factory - Caf...|11/20/2016 1 chec...| 4.0 star rating |         4|      HIGH|        1|           0|        2|       0|           0|           0|        -2|           0|            0|           0|          0|        0|         0|           -1|         0|\n",
            "|The Factory - Caf...|\"10/27/2016 2 che...| 4.0 star rating |         4|      HIGH|        1|           3|        0|       0|           0|           2|         0|           0|            0|           0|          1|        1|         1|            0|         0|\n",
            "|The Factory - Caf...|\"11/2/2016 2 chec...| 5.0 star rating |         5|      HIGH|        1|           0|        1|       0|           1|          -1|         0|           1|            1|           0|         -1|        0|         0|            0|         0|\n",
            "|The Factory - Caf...|\"10/25/2016 1 che...| 3.0 star rating |         3|       LOW|        0|           3|        3|       0|           0|           0|         1|           0|            0|           0|          0|        2|         0|            1|         0|\n",
            "|The Factory - Caf...|11/10/2016 3 chec...| 5.0 star rating |         5|      HIGH|        1|           3|        1|       0|           0|           0|         0|           0|            0|           0|          1|        0|         0|            0|         0|\n",
            "+--------------------+--------------------+-----------------+----------+----------+---------+------------+---------+--------+------------+------------+----------+------------+-------------+------------+-----------+---------+----------+-------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHrs8Uk1i0RX",
        "colab_type": "text"
      },
      "source": [
        "## Transform DataFrame to fit review_rating table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvpPZ1TEiwyu",
        "colab_type": "code",
        "outputId": "b195157c-c6e4-4690-dc2d-7e506d290765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "review_df = df.select([\"review_text\",\"num_rating\"])\n",
        "review_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------+\n",
            "|         review_text|num_rating|\n",
            "+--------------------+----------+\n",
            "|11/25/2016 1 chec...|         5|\n",
            "|12/2/2016 Listed ...|         4|\n",
            "|11/30/2016 1 chec...|         4|\n",
            "|11/25/2016 Very c...|         2|\n",
            "|12/3/2016 1 check...|         4|\n",
            "|11/20/2016 1 chec...|         4|\n",
            "|\"10/27/2016 2 che...|         4|\n",
            "|\"11/2/2016 2 chec...|         5|\n",
            "|\"10/25/2016 1 che...|         3|\n",
            "|11/10/2016 3 chec...|         5|\n",
            "|\"10/22/2016 1 che...|         4|\n",
            "|11/20/2016 The st...|         3|\n",
            "|11/17/2016 1 chec...|         3|\n",
            "|12/5/2016 This is...|         5|\n",
            "|11/13/2016 Beauti...|         5|\n",
            "|11/9/2016 1 check...|         5|\n",
            "|11/6/2016 Really ...|         5|\n",
            "|10/25/2016 1 chec...|         4|\n",
            "|10/15/2016 1 chec...|         4|\n",
            "|12/1/2016 So much...|         4|\n",
            "+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BTqDdJniwwG",
        "colab_type": "code",
        "outputId": "ab80cba5-933c-4f7e-e8bf-573b4f074b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "from pyspark.sql.functions import regexp_extract, length\n",
        "review_df = df.withColumn(\"date\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+\", 0))\\\n",
        "      .withColumn(\"review_text\", regexp_extract(\"review_text\", \"\\d+/\\d+/\\d+(?:\\s)(.*)\", 1))\\\n",
        "      .withColumnRenamed(\"num_rating\", \"label\")\\\n",
        "      .select([\"label\", \"date\", \"review_text\"])\n",
        "review_df = review_df.withColumn('review_length', length(review_df['review_text'])).dropna()\n",
        "review_df.cache()\n",
        "review_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+--------------------+-------------+\n",
            "|label|      date|         review_text|review_length|\n",
            "+-----+----------+--------------------+-------------+\n",
            "|    5|11/25/2016|1 check-in Love l...|          542|\n",
            "|    4| 12/2/2016|Listed in Date Ni...|          279|\n",
            "|    4|11/30/2016|1 check-in Listed...|         1240|\n",
            "|    2|11/25/2016|Very cool vibe! G...|          364|\n",
            "|    4| 12/3/2016|1 check-in They a...|          629|\n",
            "|    4|11/20/2016|1 check-in Very c...|          999|\n",
            "|    4|10/27/2016|2 check-ins Liste...|         1326|\n",
            "|    5| 11/2/2016|2 check-ins Love ...|         1780|\n",
            "|    3|10/25/2016|1 check-in Ok let...|         1805|\n",
            "|    5|11/10/2016|3 check-ins This ...|          725|\n",
            "|    4|10/22/2016|1 check-in Listed...|         1669|\n",
            "|    3|11/20/2016|The store has A+ ...|          509|\n",
            "|    3|11/17/2016|1 check-in Listed...|          835|\n",
            "|    5| 12/5/2016|This is such a cu...|          152|\n",
            "|    5|11/13/2016|Beautiful eccentr...|          378|\n",
            "|    5| 11/9/2016|1 check-in Listed...|         1452|\n",
            "|    5| 11/6/2016|Really love the v...|          395|\n",
            "|    4|10/25/2016|1 check-in Check ...|          778|\n",
            "|    4|10/15/2016|1 check-in Note: ...|          843|\n",
            "|    4| 12/1/2016|So much aesthetic...|          215|\n",
            "+-----+----------+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nRtLVc7i8gK",
        "colab_type": "text"
      },
      "source": [
        "## Create Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPFN7XcGiwtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "# Create all the features to the data set\n",
        "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"token_text\")\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "hashingTF = HashingTF(inputCol=\"token_text\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxsBBQRiiwk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "\n",
        "# Create feature vectors\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'review_length'], outputCol='features')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ6f6eXfjDfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZkNHtAjGUS",
        "colab_type": "text"
      },
      "source": [
        "## Transform DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbq2d16jjDcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(review_df)\n",
        "cleaned = cleaner.transform(review_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAIOltOojKal",
        "colab_type": "code",
        "outputId": "b84e4249-ae5d-4b64-decd-04e85e4cf9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "# Show label of ham spame and resulting features\n",
        "cleaned.select(['label', 'features']).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|    5|(262145,[9639,991...|\n",
            "|    4|(262145,[512,1588...|\n",
            "|    4|(262145,[3578,963...|\n",
            "|    2|(262145,[9639,157...|\n",
            "|    4|(262145,[3294,736...|\n",
            "|    4|(262145,[14,8443,...|\n",
            "|    4|(262145,[14,604,3...|\n",
            "|    5|(262145,[14,4543,...|\n",
            "|    3|(262145,[3890,392...|\n",
            "|    5|(262145,[991,2437...|\n",
            "|    4|(262145,[14,326,3...|\n",
            "|    3|(262145,[6922,736...|\n",
            "|    3|(262145,[6922,963...|\n",
            "|    5|(262145,[4081,158...|\n",
            "|    5|(262145,[1076,199...|\n",
            "|    5|(262145,[14,329,1...|\n",
            "|    5|(262145,[14,1998,...|\n",
            "|    4|(262145,[14,5281,...|\n",
            "|    4|(262145,[7388,963...|\n",
            "|    4|(262145,[9639,158...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S37P7oA2jMF9",
        "colab_type": "text"
      },
      "source": [
        "## Run NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_KHNZkqjN-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSO42Oc9jPRV",
        "colab_type": "code",
        "outputId": "7f41cb6a-6cac-4b51-ccb4-b75e19416b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Tranform the model with the testing data\n",
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|label|     date|         review_text|review_length|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+---------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|    1|1/10/2015|1 check-in I just...|         1444|[1, check-in, i, ...|[1, check-in, edi...|(262144,[14,2326,...|(262144,[14,2326,...|(262145,[14,2326,...|[-8530.8523111699...|[5.18392012015807...|       4.0|\n",
            "|    1|1/14/2016|Worst brownie I h...|          103|[worst, brownie, ...|[worst, brownie, ...|(262144,[9639,244...|(262144,[9639,244...|(262145,[9639,244...|[-871.82807074224...|[2.00505882381113...|       3.0|\n",
            "|    1|1/19/2016|Its good place to...|          440|[its, good, place...|[good, place, stu...|(262144,[14,9639,...|(262144,[14,9639,...|(262145,[14,9639,...|[-3113.7191459700...|[1.31283836251754...|       3.0|\n",
            "|    1|1/28/2014|Beware of them ti...|           61|[beware, of, them...|[beware, tipping,...|(262144,[9639,341...|(262144,[9639,341...|(262145,[9639,341...|[-552.67910884004...|[2.43672671168775...|       3.0|\n",
            "|    1| 1/9/2016|Thoroughly disapp...|          358|[thoroughly, disa...|[thoroughly, disa...|(262144,[9639,134...|(262144,[9639,134...|(262145,[9639,134...|[-2222.6114818134...|[3.08210704244631...|       3.0|\n",
            "+-----+---------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tik9-ZskjPvO",
        "colab_type": "text"
      },
      "source": [
        "## Predict accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuFyog8-jQCQ",
        "colab_type": "code",
        "outputId": "65515fea-6303-4db1-d459-d823d9c5608f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Use the Class Evaluator for a cleaner description\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model at predicting reviews was: 0.141092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vRR-utfj1yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}